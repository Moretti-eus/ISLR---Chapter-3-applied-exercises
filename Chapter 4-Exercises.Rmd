---
title: "Chapter 4- Exercises"
output: html_document
---


#Question 10
```{r}
library(ISLR)

attach(Weekly)

##A)


cor(Weekly[,-9])
pairs(Weekly, col=Weekly$Direction)

###volume and year appears to be highly correlated (0,84%)


##B)
lgm1 <- glm(Direction ~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family=binomial)
summary(lgm1)

###None of the variables appear to be significant

##C)COnfusion matrix e mean

lgm.pred = predict(lgm1, type="response")
lgm.prob = ifelse(lgm.pred>0.5 , "Up", "Down")
table(lgm.prob, Direction)
mean(lgm.prob==Direction) ###56% is the percentage of correct predictions and 43,8% is the error rate. when the market goes up, the model is right 92% and when goes down is 11,1%.


#D) Logistic Regression using train and test
train = (Year<2009)


lgm.fit2 = glm(Direction ~Lag2, family=binomial, subset=train)
summary(lgm.fit2)

Direction

lgm.pred1 = predict(lgm.fit2, newdata=Weekly[!train,], type="response") #argumento "newdata" deve ser uma matriz
lgm.prob1 = ifelse(lgm.pred1>0.5, "Up", "Down")
table(lgm.prob1, Direction[!train])
mean(lgm.prob1==Direction[!train])

###37,5% is the test error rate

#E)LDA

lda.1 = lda(Direction~Lag2, subset=train)
lda.1


lda.pred.1 = predict(lda.1, Weekly[!train,], type="response") #nesse caso como o predict gera uma lista (eu acho que faz isso por causa da funçao LDA, que por exemplo, nem precisa de sumamry pra ver os resultados depois). sendo assim, nessa lista tem o elemnto class que é analogo a funçao ifelse que vinha sendo colocada

table(lda.pred.1$class, Direction[!train])
mean(lda.pred.1$class==Direction[!train])

###37,5% error rate

##F)QDA

qda.1 = qda(Direction ~Lag2, subset=train)
qda.1

qda.pred.1 = predict(qda.1, Weekly[!train,], type="response")#notar que aqui de novo nao precisa-se puxar a funcao de ifelse, como no glm, porque a funcao de fit do qda é analoga a de lda
table(qda.pred.1$class, Direction[!train])

###41,3% is the error rate, model chooses "up" all the time

##G)
train.x = as.matrix(Lag2[train])
test.x= as.matrix(Lag2[!train])
train.y = Direction[train]

set.seed(1)
knn.fit.1 = knn(train.x , test.x , train.y, k=1) #entradas de knn, em relaçao aos previsores, é tudo em forma matricial
table(knn.fit.1, Direction[!train])








```






#Question 11
```{r}
attach(Auto)

##A)Create a binary variable, mpg01
median_mpg <- median(mpg)
mpg01 <- ifelse(mpg>median_mpg, 1, 0)
auto01 <-cbind(Auto, mpg01)

##B)Explore the data graphically

pairs(auto01, col=mpg01)
cor(auto01[,-9])
skim(auto01)###função muito util para visualizar dados, é basicamente um summary() melhor e diz sobre missing data
vis_dat(auto01)###função para visualizar missing data e que tipos de variaveis existem no df


###cylinders, horsepower, acceleration, year, origin look like the most useful to predict, due to correlation. there are many variables correlated among themselves.

##C)Split the data into a training set and a test set.

auto_train <- auto01[1:313,] ###firste 80% of obs
auto_test <- auto01[314:392,]###last 20%
test_mpg01 <-as.data.frame(auto_test$mpg01) 
train_mpg01 <-auto_train$mpg01

##D)Perform LDA on the training data in order to predict mpg01

lda.q11 <- lda(mpg01 ~cylinders+horsepower+acceleration+year+origin, data=auto_train)
lda.q11

lda.q11.prev <- predict(lda.q11, auto_test)###segundo termo do predict deve ser uma matriz com os dados de teste
table(lda.q11.prev$class, auto_test$mpg01)
mean(lda.q11.prev$class==auto_test$mpg01)

    ###all 0 results are calculated and 69/74 of 1 observations are predicted. 93% is the percentage of correct predictions

##E)Perform QDA on the training data in order to predict mpg01
qda.q11 <- qda(mpg01~cylinders+horsepower+acceleration+year+origin, data=auto_train)
qda.q11

qda.q11.pred <- predict(qda.q11, auto_test)
table(qda.q11.pred$class, auto_test$mpg01)
mean(qda.q11.pred$class==auto_test$mpg01)

  ###all 0 results are calculated and 69/74 of 1 observations are predicted. 93% is the percentage of correct predictions


##F) Perform logistic regression on the training data
glm.q11 <- glm(mpg01~cylinders+horsepower+acceleration+year+origin, data=auto_train, family=binomial)
summary(glm.q11)

glm.q11.pred <- predict(glm.q11, auto_test, type = "response")
glm.q11.prob <- ifelse(glm.q11.pred>0.5, 1, 0)
table(glm.q11.prob, auto_test$mpg01)
mean(glm.q11.prob==auto_test$mpg01)

  #one 0 result is missed, 68/74 of 1 observations are predicted. 91% is the percentage of correct predictions

##G)Perform KNN on the training data, with several values of K

knn.q11 <- knn(auto_train[,-9], auto_test[,-9], train_mpg01, k=3)###Estava dando erro porque nao aceita variaveis as factor!tive que tirar a coluna de nomes

table(knn.q11, auto_test$mpg01)
mean(knn.q11==auto_test$mpg01)

    ###all 0 results are calculated and 54/74 of 1 observations are predicted. 74% is the percentage of correct predictions

```

#Question 12

```{r}
##A)Write a function, Power(), that prints out the result of raising 2 to the 3rd power. 

power_my <- function(){
  print(2^3)
}


##B) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a.

power_my2 <- function(x,a){
  print(x^a)
}

##C)Using the Power2() function that you just wrote, compute 103, 817, and 1313


##D)Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen.

power_my3 <- function(x,a){
  result=x^a
  return(result)
}

##E) Now using the Power3() function, create a plot of f(x) = x2

x <- c(1:10)
y <- power_my3(x,2)
plot(x,y)

##F) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x

power_my4 <- function(x,y){
  result=x^y
  plot(x, result)
}

```

#Question 13

```{r}
attach(Boston)

med_crim <- median(crim)
crim_01 <- ifelse(crim>med_crim, 1, 0)
Boston01 <- cbind(Boston, crim_01)

train <- 1:(length(crim) / 2)
test <- (length(crim) / 2 + 1):length(crim)
Boston.train <- Boston01[train, ]
Boston.test <- Boston01[test, ]
crim01.test <- crim_01[test]

fit.lda <- lda(crim_01 ~ . - crim_01 - crim, data = Boston01)
pred.lda <- predict(fit.lda, Boston.test)
table(pred.lda$class, crim01.test)
mean(pred.lda$class != crim01.test)

```





